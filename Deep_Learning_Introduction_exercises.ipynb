{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7744aade-a4d3-47c5-846d-7595585d0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "from IPython.display import display\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c615034-d362-45ec-b6ca-12d7335da3f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic structure of artificial neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103f97dc-489a-477b-8c61-0c8e83d6a705",
   "metadata": {},
   "source": [
    "The conceptual foundations of **[artificial neural networks](https://en.wikipedia.org/wiki/Neural_network_(machine_learning))** go back to the work of [Warren McCulloch](https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch) and [Walter Pitts](https://en.wikipedia.org/wiki/Walter_Pitts), who proposed linked networks for spatial pattern recognition in analogy to neurons as early as $1943$. In 1958, [Frank Rosenblatt et al.](https://en.wikipedia.org/wiki/Frank_Rosenblatt) achieved the first practical implementation of a neural network in the form of the **[perceptron](https://en.wikipedia.org/wiki/Perceptron)**. In 1969, criticism by [Marvin Minsky](https://en.wikipedia.org/wiki/Marvin_Minsky) of the inability to solve non-linearly separable problems (such as the **[XOR problem](https://en.wikipedia.org/wiki/XOR_gate)**) with simple perceptrons led to a temporary decline in research interest (the so-called AI winter). This changed in the 1980s, when various advances in AI research, such as the method of **[backpropagation](https://en.wikipedia.org/wiki/Backpropagation)**, showed that multilayer perceptrons are also capable of solving non-linearly separable problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7269dd9-cecc-4b0b-97f6-d1ea334affdf",
   "metadata": {},
   "source": [
    "We have talked in detail elsewhere about the different types of machine learning - **[unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning)**, **[supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)** and **[reinforcement learning](https://de.wikipedia.org/wiki/Best%C3%A4rkendes_Lernen)**. We have presented various machine learning algorithms that are used for the different types of learning. Neural networks are characterized in particular by the fact that they can be used successfully in all three types of learning with appropriate preparation. This universal applicability also explains the increased use of artificial neural networks in a wide variety of areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb975ce3-db32-43dd-bac5-522a825c642c",
   "metadata": {},
   "source": [
    "In this workshop we will look at the strengths and weaknesses of **neural networks** as well as their possible applications and the underlying mathematical formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c33fef-d108-446f-b0bb-4d00af7728ea",
   "metadata": {},
   "source": [
    "## Structure of the neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd2a90d-01cc-4cbb-acbb-be431fc688e8",
   "metadata": {},
   "source": [
    "Let's first take a look at the model for neural networks: the nerve cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de72424b-feab-4cae-b038-5edfe94990e8",
   "metadata": {},
   "source": [
    "<img src=\"./images_en/realistische-neuronenanatomie_1284-68077.avif\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7812f0-f736-4c84-b973-3f9d2ad286b1",
   "metadata": {},
   "source": [
    "As shown in the figure above, a nerve cell consists of **dendrites**, **soma** and **axon**. The **dendrites** absorb messenger substances, the so-called neurotransmitters, when these are released by excited neighboring nerve cells. The connections between the dendrites and the axon of the preceding cell are called **synapses**. The neuron has a membrane potential that initially suppresses the transmission of a nerve stimulus. Only when a certain **excitation threshold** is exceeded is an **action potential** triggered and a nerve stimulus transmitted to the next cell via the **axon**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208e003-d833-4fbc-8596-382eb5e52fef",
   "metadata": {},
   "source": [
    "When the neuron is stimulated, the so-called **[All-or-none_law](https://en.wikipedia.org/wiki/All-or-none_law)** applies, which states that either a stimulus is triggered completely or not at all.\n",
    "\n",
    "This can be expressed mathematically with the **Heaviside step function**, which is defined as\n",
    "\n",
    "$$H(x) \\begin{cases} x \\lt 0 \\cdots 0 \\\\ x \\ge 0 \\cdots 1 \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eacdfab-f0b3-44d4-bdc5-e72b681fc59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heaviside step function\n",
    "def heaviside(x):\n",
    "    return 0.5 * (np.sign(x) + 1)\n",
    "\n",
    "\n",
    "# Generate values for x\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# Calculate the values of the heaviside step function\n",
    "y = heaviside(x)\n",
    "\n",
    "# Plot\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.tight_layout()\n",
    "plt.plot(x, y, linewidth = 2.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('H(x)')\n",
    "plt.title('Heaviside step function')\n",
    "plt.ylim([-0.5,1.5])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31c2a0-4745-455d-931c-9f5f64101394",
   "metadata": {},
   "source": [
    "In biological terms, we therefore have a **variable stimulus**, a **threshold value** that must be exceeded and an **activation function** to trigger the stimulus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b1f28-5665-4edd-a691-dea3fc853255",
   "metadata": {},
   "source": [
    "## Simple perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d00a2c-3edf-4eed-b030-8cf8e98bba26",
   "metadata": {},
   "source": [
    "Next, let's look at the simplest structure of a neural network, the **peceptron** originally proposed by Rosenblatt. This consists of two inputs, the neuron itself and one output, as shown in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff5378-20e9-46e1-a0e6-b727b1aad7ad",
   "metadata": {},
   "source": [
    "<img src=\"./images_en/perceptron.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e7a1e-92ce-4bea-a5cc-f0c0d068e16f",
   "metadata": {},
   "source": [
    "The structure of **artificial neural networks** follows the basic principle of a **biological nervous system**. In neural networks, neurons correspond to the **[artificial neurons](https://en.wikipedia.org/wiki/Artificial_neuron)** or nodes in the network. These neurons are the basic processing units. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa71c1c1-3d67-4566-833f-8447a26124ff",
   "metadata": {},
   "source": [
    "Artificial neural networks consist of individual neurons that are arranged in so-called layers. The first layer or *input layer* consists of the input values, followed by further layers of neurons, the so-called *hidden layers*, and finally an *output layer*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed446f5-95a4-42f2-bfb3-88e45bfa4c94",
   "metadata": {},
   "source": [
    "In analogy to biological neurons, which are activated by stimulation above a certain threshold value to transmit stimuli, in neural networks the weighted sum of the inputs is passed on to connected neurons. In relation to a neuron, this results in:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e0a94-0919-4fcc-8e29-e968cf61b6ce",
   "metadata": {},
   "source": [
    "$$ \\text{Input} = \\sum_{i=1}^N x_i w_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ba5da-1942-4a7e-b588-ef8908396ecd",
   "metadata": {},
   "source": [
    "The $x_i$ are the individual input values and the $w_i$ are the respective weights of the $N$ inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46b698-727f-4c3f-9c29-0a06ba267190",
   "metadata": {},
   "source": [
    "Equivalent to the biological threshold at which a neuron is activated to transmit a signal, we can add a threshold value $b$, the so-called bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2e45d-5ee0-4eac-b8a0-507cc908dd45",
   "metadata": {},
   "source": [
    "$$ \\text{Input} = \\sum_{i=1}^N x_i w_i + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce353026-b2bc-4f71-b951-3290df52578c",
   "metadata": {},
   "source": [
    "We still normally have to apply a suitable activation function to the output calculated in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211bd24-ed09-405a-8ccb-8fc736d94e50",
   "metadata": {},
   "source": [
    "$$f_{\\text{Aktiv}}(\\text{Input}) = f_{\\text{Aktiv}}(\\sum_{i=1}^N x_i w_i + b) = \\text{Output}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba5f5a9-5530-4c3f-867e-b8248556e7da",
   "metadata": {},
   "source": [
    "The **activation function** maps the weighted sum of the input values to a specific value range and is used to control the output of a neuron or a layer. It decides whether and to what extent a neuron is activated and what information is passed on to the next layers. In addition, suitable activation functions make it possible to describe non-linear relationships. We will come back to the exact form of different activation functions later and for the moment we will assume linearly transmitted values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162fccf8-2169-4121-b063-c207f49afdd4",
   "metadata": {},
   "source": [
    "So mathematically speaking \n",
    "\n",
    "$$f_{\\text{Active}}(\\text{Input}) = \\text{Input}$$ \n",
    "\n",
    "and thus the following applies\n",
    "\n",
    "$$ \\text{Input} = \\sum_{i=1}^N x_i w_i + b = \\text{Output}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8641464-c174-43d2-9455-feafe02fad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure: Weights and bias\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "# Values for weight and bias\n",
    "weights = [3, 2, 1]  # Weight values for the first plot\n",
    "bias_values = [0]  # Bias for the first plot (bias = 0)\n",
    "weight_constant = 1  # Constant value for weight in the second plot\n",
    "bias_values_2 = [1, 0, -1]  # Different bias values for the second plot\n",
    "\n",
    "# Create two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), dpi = 600)\n",
    "\n",
    "# Plot for the weight values with bias = 0\n",
    "for weight in weights:\n",
    "    ax1.plot([-2, -1, 0, 1, 2], [weight * x + 0 for x in [-2, -1, 0, 1, 2]], label=f\"Weight = {weight}\")\n",
    "ax1.set_title(\"Different weights with constant bias\")\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y = weight * x + bias\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "ax1.set_xticks(np.arange(-2,3,1))\n",
    "ax1.set_yticks(np.arange(-6,7,1))\n",
    "ax1.axhline(0, 2, color='black',linewidth=1)\n",
    "ax1.axvline(0, 6, color='black',linewidth=1)\n",
    "ax1.annotate('', xy=(2, 0), xytext=(0, 0), arrowprops=dict(facecolor='black', edgecolor='black', arrowstyle='->', lw=1))\n",
    "ax1.annotate('', xy=(0, 6), xytext=(0, 0), arrowprops=dict(facecolor='black', edgecolor='black', arrowstyle='->', lw=1))\n",
    "\n",
    "\n",
    "# Plot for constant weight = 1 and different bias values\n",
    "for bias in bias_values_2:\n",
    "    ax2.plot([-2, -1, 0, 1, 2], [weight_constant * x + bias for x in [-2, -1, 0, 1, 2]], label=f\"Bias = {bias}\")\n",
    "ax2.set_title(\"Different bias at constant weight\")\n",
    "ax2.set_xlabel(\"x\")\n",
    "ax2.set_ylabel(\"y = weight * x + bias\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "ax2.set_xticks(np.arange(-2,3,1))\n",
    "ax2.set_yticks(np.arange(-4,7,1))\n",
    "ax2.axhline(0, 2, color='black',linewidth=1)\n",
    "ax2.axvline(0, 5, color='black',linewidth=1)\n",
    "ax2.annotate('', xy=(2, 0), xytext=(0, 0), arrowprops=dict(facecolor='black', edgecolor='black', arrowstyle='->', lw=1))\n",
    "ax2.annotate('', xy=(0, 5), xytext=(0, 0), arrowprops=dict(facecolor='black', edgecolor='black', arrowstyle='->', lw=1))\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45950b56-9411-454c-a513-aeea36fa81a7",
   "metadata": {},
   "source": [
    "Let's look at a simple example in `Python` to understand some of the mathematical concepts involved in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2fec7b-ed90-4372-a793-7a3d6f61eceb",
   "metadata": {},
   "source": [
    "## Simple neural network in `Python`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d4fbf-755a-4b8e-9c4e-8bcef87f38c6",
   "metadata": {},
   "source": [
    "<img src=\"./images_en/einfaches_netz1.png\" alt=\"drawing\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b6c56-63ba-4be7-ae87-86f598296bf0",
   "metadata": {},
   "source": [
    "We first try to model the input layer of a network consisting of a neuron with two input values $x_1, x_2$, two weights $w_1, w_2$ and a bias $b$ in `Python`. During the first run of the network, the input values are first passed to the input layer and the weights and bias are initialized randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb6ed7-ffca-4c0d-b3d9-34fc4f065dd5",
   "metadata": {},
   "source": [
    "### Forward propagation of the network in `Python`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c55a027-6555-448a-b9d4-604bcea87b39",
   "metadata": {},
   "source": [
    "We start by creating the input layer and model the calculation of the output of the first neuron. This corresponds to the first step of forward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11980b4a-cc7d-4817-8e83-226c5f43d78b",
   "metadata": {},
   "source": [
    "$$\\text{Input} = \\sum_{i=1}^2 x_i w_i + b = x_1 w_1 + x_2 w_2 + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcdca7c-5286-449d-ac8b-7830236e39ba",
   "metadata": {},
   "source": [
    "### Feedforward propagation - input layer with one neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d927512-10f2-4027-be2a-c61007e43816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input values\n",
    "inputs = [5, 7]\n",
    "\n",
    "# Randomly initialized weights\n",
    "W1 = [6, 4]\n",
    "\n",
    "# Randomly initialized bias\n",
    "b1 = 3\n",
    "\n",
    "# Activation\n",
    "A1 = inputs[0] * W1[0] + inputs[1] * W1[1] + b1\n",
    "A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024771a-5872-481d-9da7-3e89837a5a41",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Calculate the activation of a neuron for the input values $10$ and $12$, the weights of the input layer $1$ and $2$ and a bias of $5$. How can we extend the model to three input values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57728e91-8153-4554-8cd3-6e8fe8b15e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e0959-1641-4fdc-9405-f0d57ec03578",
   "metadata": {},
   "source": [
    "Let's generalize our model to two neurons in the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f0672-52b5-49cc-85a0-4404e85bfacf",
   "metadata": {},
   "source": [
    "### Feedforward propagation - hidden layer with two neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a60da-1279-46f1-91df-09760d43df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure: two neurons in the hidden layer\n",
    "# Slides\n",
    "image_paths = [\"./images_en/w_b1.png\", \"./images_en/w_b2.png\", \"./images_en/w_b3.png\", \"./images_en/w_b4.png\", \"./images_en/w_b5.png\", \"./images_en/w_b6.png\", \"./images_en/w_b7.png\", \"./images_en/w_b8.png\"]\n",
    "\n",
    "# Selection of images\n",
    "def show_image(index):\n",
    "    img = mpimg.imread(image_paths[index])\n",
    "    plt.figure(figsize=(2, 1), dpi = 600)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Create slider widget\n",
    "slider = widgets.IntSlider(min=0, max=len(image_paths) - 1, step=1, description='Picture')\n",
    "widgets.interactive(show_image, index=slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307ce76-7a3c-4f82-a39c-42b6397bbf74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input values\n",
    "inputs = [5, 7]\n",
    "\n",
    "# Randomly initialized weights for inputs in neuron 1\n",
    "W1 = [6, 4]\n",
    "\n",
    "# Randomly initialized bias for inputs in neuron 1\n",
    "b1 = 3\n",
    "\n",
    "# Randomly initialized weights for inputs in neuron 2\n",
    "W2 = [5, 3]\n",
    "\n",
    "# Randomly initialized bias for inputs in neuron 2\n",
    "b2 = -5\n",
    "\n",
    "A1 = inputs[0] * W1[0] + inputs[1] * W1[1] + b1\n",
    "     \n",
    "A2 = inputs[0] * W2[0] + inputs[1] * W2[1] + b2\n",
    "\n",
    "print('Output of neuron 1:', A1)\n",
    "print('Output of neuron 2:', A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a901f-8109-47fc-bb1c-125de8e1954a",
   "metadata": {},
   "source": [
    "This can be simplified by summarizing all weights of a layer into a matrix and specifying the inputs and biases as column vectors. We therefore calculate the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beecf906-3148-467b-817c-647e927c5324",
   "metadata": {},
   "source": [
    "$$\n",
    "A_1\n",
    "=\n",
    "W_1 \\cdot \\vec{X} + \\vec{b_1}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "6 & 4 \\\\\n",
    "5 & 3\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "5  \\\\\n",
    "7 \n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "3  \\\\\n",
    "-5 \n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "61 \\\\\n",
    "41\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabfff82-b46a-426f-9e08-d051a4b967ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input values\n",
    "X = np.array([[5],[7]])\n",
    "# Weights of the input layer as a matrix\n",
    "W1 = np.array([[6, 4],[5, 3]])\n",
    "# Biases of the input layer as a column vector\n",
    "b1 = np.array([[3],[-5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f160c-8043-4a7d-8185-a6cfe2695b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output hidden layer\n",
    "W1 @ X + b1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee7815a-07a2-4383-9f4e-163fd79c08e3",
   "metadata": {},
   "source": [
    "### Vectors and matrices in `NumPy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4fc196-4e87-4392-88d3-6f71f0f7c78a",
   "metadata": {},
   "source": [
    "In this section we deal with the framework `NumPy`. `NumPy` is specialized for numerical calculations in `Python` and covers many areas of mathematics. Let's see how we can use `NumPy` to do linear algebra. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f26b1-44ad-48d2-9196-13ad73c63993",
   "metadata": {},
   "source": [
    "### Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7916d1-9584-46fb-9cd7-e3755e401be0",
   "metadata": {},
   "source": [
    "The **scalar product** of two vectors $\\vec{a}=\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix}$ and $\\vec{b}=\\begin{pmatrix}\n",
    "4 \\\\\n",
    "5 \\\\\n",
    "6\n",
    "\\end{pmatrix}$ is given by :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a3e2b-032a-4155-ad93-0b6cf6d20554",
   "metadata": {},
   "source": [
    "$$\\vec{a} \\cdot \\vec{b}=\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "4 \\\\\n",
    "5 \\\\\n",
    "6\n",
    "\\end{pmatrix}\n",
    "=\n",
    "1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6 = 4 + 10 + 18 = 32\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b53ab-9a41-4f8d-95e5-4396b7a249c7",
   "metadata": {},
   "source": [
    "We can create arrays in `NumPy` with the `array()` function by passing rows and columns as lists. We can write to the two vectors $\\vec{a}$ and $\\vec{b}$ from the above example in `NumPy` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48277ba1-9f8a-4327-84ba-3df78d531b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "\n",
    "b = np.array([4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44393219-ff47-49c0-91c3-98336697f508",
   "metadata": {},
   "source": [
    "In `NumPy` we can calculate the scalar product of two vectors with the function `dot()` or the symbol `@`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97968132-1a82-4e20-8a83-9b09b6f371dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector or matrix multiplication\n",
    "a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b696a9-7775-4c97-8d64-1e36c3d2f3f7",
   "metadata": {},
   "source": [
    "*Note*: An $n$-dimensional array is not distinguished in `NumPy` in the mathematical sense of row or column vector without further specification, but is interpreted according to the so-called broadcasting rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb39dcc-ff1a-4a01-8a33-bbea8ae651da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d96a5-052d-4b81-862e-fb14153f958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e8319-371e-47c7-b8bb-6eb671ef960e",
   "metadata": {},
   "source": [
    "### Example: Dyadic product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7d08e0-a073-43a0-aacd-34cccb141f18",
   "metadata": {},
   "source": [
    "With Broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fc3a5-e9f7-43d3-9088-056b8ccc48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "print('Shape_a', a.shape)\n",
    "print('Shape_b', b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c87fd0-a89e-4284-93b6-acbb7b39094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21e077-e9b2-4691-aa9c-7bd88afb6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "b @ a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca41ca6-1ac3-41b1-a34e-d0c1b35c1cf6",
   "metadata": {},
   "source": [
    "With vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9758e61-3e41-4a81-aa66-f5fa47628206",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1],[2],[3]])\n",
    "b = np.array([[4,5,6]])\n",
    "print('Shape_a', a.shape)\n",
    "print('Shape_b', b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4595fb27-ca76-4856-9980-d0b481ff5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f37e64-951c-405f-b108-63c8011ac0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b @ a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26301276-5b48-4af5-883f-085aff60504d",
   "metadata": {},
   "source": [
    "To define a vector uniquely in `NumPy` we can specify the missing dimension with additional square brackets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f221e43e-6457-4af6-a440-f6959e5c0883",
   "metadata": {},
   "source": [
    "#### Example: Column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1dbe97-cf6a-417c-a154-63409fd194b3",
   "metadata": {},
   "source": [
    "$\n",
    "\\vec{a}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "4\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff09f81-3656-4c97-b94c-9a84ea42c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_column = np.array([[1],[2],[3],[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad34a2-9c29-4447-8b6d-680f2dc22390",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_column.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28439611-47ca-4bcd-be97-ef579d02bce9",
   "metadata": {},
   "source": [
    "#### Example: Row vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722749a4-9f21-41e7-86ba-19bd09a3107e",
   "metadata": {},
   "source": [
    "$\n",
    "\\vec{b}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 & 4\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\vec{a}^T\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba41f2-9e66-4cee-83ee-077320d4c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_row = np.array([[1, 2, 3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6e4b1-fbeb-4f43-99a1-3432bf1e74a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_row.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a1377-ac9c-4d4d-8102-6ce86fcf1526",
   "metadata": {},
   "source": [
    "We can transpose vectors in `NumPy` with the syntax `vector.T`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b7b13-ef70-4a31-8e8a-c71eead6bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_column.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd61648-a5bc-4b75-9186-0118a698a5db",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "\n",
    "Calculate the scalar product $\\vec{a}^T \\cdot \\vec{b}$ for the vectors:\n",
    "\n",
    "$\n",
    "\\vec{a}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix}$\n",
    ",\n",
    "$\n",
    "\\vec{b}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "4 \\\\\n",
    "5 \\\\\n",
    "6\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aef229-515b-4c5a-b5e3-5962038ecbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdfe82-cda7-401c-b5b9-09c8be97394e",
   "metadata": {},
   "source": [
    "### Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a47534-c701-4349-8b23-8bdb8c24a097",
   "metadata": {},
   "source": [
    "A matrix is made up of column and row vectors, whereby an $(m \\times n)$ matrix consists of $m$ rows and $n$ columns. The example below therefore represents a $(3 \\times 3)$ matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca14c310-de40-45c8-a8ed-409e5dd5affa",
   "metadata": {},
   "source": [
    "$$A =\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef8762-6e8d-4ce6-bbbc-936e80b26e87",
   "metadata": {},
   "source": [
    "Note that matrices are a generalization of vectors in that they correspond to the special case of a $(n \\times 1)$ matrix. We can write to the above matrix $A$ in `NumPy` by passing rows and columns as a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181b380-61ad-4cee-928a-44294ee4e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1],[2],[3]])\n",
    "\n",
    "A = np.array([[1,2,3],\n",
    "              [4,5,6],\n",
    "              [7,8,9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e28521-64f7-40f2-bce7-3d39bbb11a99",
   "metadata": {},
   "source": [
    "### Multiplication of vectors with matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225a91fd-83d6-40b6-8f66-66f164b75b8e",
   "metadata": {},
   "source": [
    "$\\vec{a}=\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix}$ , $A =\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b36674-d5e8-40e0-af6f-61e85601ad1b",
   "metadata": {},
   "source": [
    "$A \\cdot \\vec{a} = \\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "1 \\cdot 1 + 2 \\cdot 2 + 3 \\cdot 3 \\\\\n",
    "4 \\cdot 1 + 5 \\cdot 2 + 6 \\cdot 3 \\\\\n",
    "7 \\cdot 1 + 8 \\cdot 2 + 9 \\cdot 3\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "14 \\\\\n",
    "32 \\\\\n",
    "50\n",
    "\\end{pmatrix}\n",
    "$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82bcbb-52e0-4081-9bf0-64f1c40d0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742db72-dfc8-4e29-98bc-60fc93a516b6",
   "metadata": {},
   "source": [
    "### Dimension of vectors and matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b0ef6-95bc-4ccb-9f13-bc0caa712fcb",
   "metadata": {},
   "source": [
    "### Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d20272-587e-4ace-af6d-d28057a8385f",
   "metadata": {},
   "source": [
    "Matrices have dimensions corresponding to their number of rows $m$ and columns $n$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd356ad-8e94-43b1-8979-5ab38a70bc59",
   "metadata": {},
   "source": [
    "#### Example: $4 \\times 4$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4adcbca-a840-431e-9fa0-70ef535b02ec",
   "metadata": {},
   "source": [
    "$A =\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "9 & 10 & 11 & 12 \\\\\n",
    "13 & 14 & 15 & 16\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7fef53-bc1d-499f-ba6b-8be6b4f1fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_matrix = np.array([[1,2,3,4],\n",
    "                    [5,6,7,8],\n",
    "                    [9,10,11,12],\n",
    "                    [13,14,15,16]])\n",
    "\n",
    "\n",
    "print('A_matrix:')\n",
    "print(A_matrix)\n",
    "print('')\n",
    "print('Dimensions of A_matrix:',A_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d814d-6625-465b-a9c4-2f937be4353b",
   "metadata": {},
   "source": [
    "Similar to vectors, matrices can also be transposed by swapping rows and columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0023d48-c910-42d1-97e5-14b8c7371768",
   "metadata": {},
   "source": [
    "$A^T=\n",
    "\\begin{pmatrix}\n",
    "1 & 5 & 9 & 13 \\\\\n",
    "2 & 6 & 10 & 14 \\\\\n",
    "3 & 7 & 11 & 15 \\\\\n",
    "4 & 8 & 12 & 16\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5a8e6-735a-4bb0-81cb-25138f26b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d16dfa7-0a7a-478b-9292-5c8e65d84f6d",
   "metadata": {},
   "source": [
    "*Note*: In order for vectors or matrices to be multiplied together, the left vector or matrix must have the same number of columns as the right vector or matrix has rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e8a72-122e-47d5-a432-cfaba423b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication between vector and matrix is not commutative\n",
    "print('Dimensions of a:', a.shape)\n",
    "print('Dimensions of A:', A.shape)\n",
    "#a @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfdceb9-5318-45d1-9939-95283ecaea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In general, for two matrices A * B with dimensions A = (m, n), B = (o, p),\n",
    "# that the dimension of the columns n of A must be equal to the dimension of the rows o of B\n",
    "print('Dimensions of a.T:', a.T.shape)\n",
    "print('Dimensions of A :', A.shape)\n",
    "a.T @ A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7947b1-9151-4e5a-98de-c406e510ad14",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exercise: \n",
    "\n",
    "Create the two matrices $A = \\begin{pmatrix}1 & 2 \\\\ 4 & 4 \\end{pmatrix}$ and $B=\\begin{pmatrix}5 & 6 \\\\ 7 & 8 \\end{pmatrix}$ in `NumPy` and calculate $A \\cdot B$ and $B \\cdot A$. Does $A \\cdot B = B \\cdot A$ apply?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51fb884-7602-4ab9-849d-366b2c65455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad563ed-b9fd-42f4-a4c0-b0aa2fe22a3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simple neural network with matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda721a-7f23-4d45-a3f9-572926923b08",
   "metadata": {},
   "source": [
    "Based on the previous theoretical considerations, we try to create a neural network with three input values, three neurons in the first hidden layer, four neurons in the second hidden layer and two neurons in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84379be5-75af-4e13-ad40-3ca269da7913",
   "metadata": {},
   "source": [
    "### Feedforward Propagation - Neural network with multiple layers in matrix representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd433b3-fb6c-4b48-8f9f-7597b95ee5cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initializing the weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f2a8d-77e8-497c-b7b9-c839a4a99455",
   "metadata": {},
   "source": [
    "In the practical application, the weights $W_i$ and thresholds $b_i$ of a neural network are initialized with random values at the beginning of the training phase. In `NumPy` it is possible to use the function `random.randn(m, n)` to create an array of dimension $(m \\times n)$ filled with standard normally distributed random numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2427971-b780-42c1-add9-7f2543d2a14e",
   "metadata": {},
   "source": [
    "We therefore create a column vector for this mesh with the input values $X$ of dimension $(3 \\times 1)$, the matrix $W_1$ of the weights of the first hidden layer of dimension $(3 \\times 3$ and the corresponding biases $b_1$ of dimension $(3 \\times 1)$, the matrix $W_2$ of the weights of the second hidden layer of dimension $(4 \\times 3$ and the associated biases $b_2$ of dimension $(4 \\times 1)$ and the matrix of the weights of the output layer $W_3$ of dimension $(2 \\times 4)$ and the associated biases $b_3$ of dimension $(2 \\times 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f57f0de-1989-47cb-a9cd-7df57f4448b7",
   "metadata": {},
   "source": [
    "<img src=\"./images_en/3_3_4_2_netz.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bfd566-e4e1-4a1e-845c-66f22bccd0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3 # input layer\n",
    "W1_size    = 3 # 3 neurons in the 1st hidden layer\n",
    "W2_size    = 4 # 4 neurons in the 2nd hidden layer\n",
    "W3_size    = 2 # 2 output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e0ceb-556c-4992-a9fe-dbf2752642af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[5],[7],[1]]) # Input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2fb94-8bb4-48cd-97ee-121ef5bf327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random weights and biases\n",
    "W1 = np.random.randn(W1_size, input_size) - 0.5\n",
    "b1 = np.random.randn(input_size, 1)\n",
    "W2 = np.random.randn(W2_size, W1_size) - 0.5\n",
    "b2 = np.random.randn(W2_size, 1) - 0.5\n",
    "W3 = np.random.randn(W3_size, W2_size) - 0.5\n",
    "b3 = np.random.randn(W3_size, 1) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147cc29-b2ab-4e91-a8f2-4e65d0b098ac",
   "metadata": {},
   "source": [
    "### Output hidden layer $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0311636-df78-401c-9afc-e232f0d456e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e02806-47ed-469d-9227-18f5919cceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2104cd-a95c-4910-bbde-6a71eb716cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the first hidden layer\n",
    "A1 = W1 @ X + b1\n",
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d3f85-5fcc-4c24-ab68-848265d583a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape_inputs   :',X.shape, '\\n')\n",
    "print('shape_W1       :',W1.shape, '\\n')\n",
    "print('shape_b1       :',b1.shape, '\\n')\n",
    "print('shape_A1       :',A1.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8670cb35-bc92-4a78-a8a6-0910771a9aa7",
   "metadata": {},
   "source": [
    "### Output hidden layer $2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e408d4-35a2-4aa4-8cc6-0d5fd12ff907",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6651e102-f453-4d7f-91bc-8be321bc4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d29f4-9b05-461d-ae9c-98d862837062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the second hidden layer\n",
    "\n",
    "A2 = W2 @ A1 + b2\n",
    "A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552c252-a724-42b7-816f-aafb94480da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape_A1       :',A1.shape, '\\n')\n",
    "print('shape_W2       :',W2.shape, '\\n')\n",
    "print('shape_b2       :',b2.shape, '\\n')\n",
    "print('shape_A2       :',A2.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cc5abc-352f-46e5-8fa4-de0d9c1127cb",
   "metadata": {},
   "source": [
    "### Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8fbdcd-52d4-4772-a437-47db4e387e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63bb96-c302-4dcb-9b46-339cbbc78cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa6a74-3bb2-446a-9366-d195f7884602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer\n",
    "A3 = W3 @ A2 + b3\n",
    "A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f3abf4-76a0-468e-84b8-3fc819dd9ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape_A2       :',A2.shape, '\\n')\n",
    "print('shape_W3       :',W3.shape, '\\n')\n",
    "print('shape_b3       :',b3.shape, '\\n')\n",
    "print('shape_A3       :',A3.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af905a-241b-4a88-91a3-a9948108d249",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0e8e8-714d-49c8-8875-d4d1f0e75f81",
   "metadata": {},
   "source": [
    "Let's take a brief look at another subtlety of the neural network, the activation function. These modulate the transmitted signal and ensure non-linear modulation of the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4645629-a938-4b07-a0c6-3d782088adf0",
   "metadata": {},
   "source": [
    "An essential property of activation functions is the ability to map **non-linear relationships**. If we were to use only the linear weighted sum of the inputs, we would essentially only be linking linear functions with each other, resulting in linear functions again. For example, consider the two linear functions $f(g(x)) = 3 g(x) +1$ and $g(x) = 4 x +2$. The concatenation of these functions $f(g(x))$ can then be written as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927224de-8fd5-40fc-aa85-d4bfec62cc35",
   "metadata": {},
   "source": [
    "$$ f(g(x)) = 3 g(x) + 1 = 3 (4 x + 2) + 1 = 12 x + 7 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9bc94-bab6-461f-93cf-82c646114ab6",
   "metadata": {},
   "source": [
    "The result is again a linear function! On the other hand, a sufficiently large artificial neural network with non-linear activation functions is able to approximate any continuous function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f22c1-ce9f-401d-b4ff-e80bc34fccf4",
   "metadata": {},
   "source": [
    "A frequently used activation function in this context is the **[sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function)** known from logistic regression:\n",
    "\n",
    "$$A(z) = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94a0b1-d851-468d-a9d2-209cf6c224ce",
   "metadata": {},
   "source": [
    "Where $A(z)$ is the activation of the neuron and $z = \\sum_i w_i x_i +b$ is the weighted sum of the inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba87ea1-2d8c-4c9b-948f-5bf0a2c1a2fe",
   "metadata": {},
   "source": [
    "The sigmoid function is used both in hidden layers and as an output activation function in **binary classification networks**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de52f48-02df-4ea7-8ea6-54dbf0f25044",
   "metadata": {},
   "source": [
    "An activation function similar to the sigmoid function is the **[hyperbolic tangent](https://en.wikipedia.org/wiki/Hyperbolic_functions)**:\n",
    "\n",
    "$$\\tanh (z) = \\frac{\\sinh (z)}{\\cosh (z)} = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15c382-7cf9-405d-a7a1-a30f8cbf080e",
   "metadata": {},
   "source": [
    "Both functions have in common that they restrict the range of values to which they map. In the case of the logistic function, this maps any values from $\\mathbb{R}$ to the interval $[ 0 \\ $ ,$ \\ 1 ]$. In the case of the hyperbolic tangent, this maps values from $\\mathbb{R}$ to the interval $[ -1 \\ $ ,$ \\ 1 ]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767514fa-e294-465d-bbd5-871a426e48a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "The calculations of the sigmoid function and the tangent hyperbolic function are comparatively time-consuming operations, as the terms $e^{\\pm z}$ have to be calculated and divided. \n",
    "\n",
    "The **[rectifier function](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))** (*rectified linear unit, ReLU*) serves as an alternative. This has the advantage that it is easier to calculate than some other activation functions such as the sigmoid function or the tangent hyperbolic function. It also helps to avoid the vanishing gradient problem that can occur with deep neural networks. The **ReLU function** is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5697f41-2994-4d5c-be9e-44d8c972b254",
   "metadata": {},
   "source": [
    "$$ A(z) = max(0,z) \\begin{cases}\n",
    "z & \\text{for} \\ z \\gt 0, \\\\ 0 & \\text{else}\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10737ed-c5f0-4d08-8dd2-fcddfb3616af",
   "metadata": {},
   "source": [
    "Although there is no slope for $z < 0$ in the values of the ReLU function, ReLU activation functions in artificial neural networks show very good optimization performance and are now one of the most commonly used activation functions in deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a701e93-9c9d-4f77-9c28-e0af2afbd006",
   "metadata": {},
   "source": [
    "An extension of the ReLU function is the **[Leaky ReLU function](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Leaky_ReLU)**. The idea behind it is that negative values for $z$ also have a low gradient in order to avoid the so-called **[vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)**. The Leaky-ReLU function is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4fdef5-bb10-449e-9cda-1e4331494c9a",
   "metadata": {},
   "source": [
    "$$ A(z) \\begin{cases}\n",
    "z & \\text{for} \\ z \\gt 0, \\\\ 0,01 \\cdot z & \\text{else}\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09453fc-7b12-4c60-b947-2d96de57ce0f",
   "metadata": {},
   "source": [
    "Another important activation function for the **output layer of classification networks** is the **[softmax function](https://en.wikipedia.org/wiki/Softmax_function)**. This is used to calculate the probability distribution of $K$ in different possible classes for **multi-class classification**. The softmax function is defined in *component notation* as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100159a-01bc-436b-9658-6020a4b8b81e",
   "metadata": {},
   "source": [
    "$$ A(z_j) = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a88c99-a8bd-4024-a54e-b6ecc201f95b",
   "metadata": {},
   "source": [
    "For three classes, the **Softmax function** would be an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ee81d-11ff-43c3-a910-db65332126eb",
   "metadata": {},
   "source": [
    "$$ A(z_1) = \\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}} \\ , \\   A(z_2) = \\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}} \\ , \\   A(z_3) = \\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c0a353-19de-439b-a2ee-01f196d7f774",
   "metadata": {},
   "source": [
    "The sum of the individual components adds up to $1$ in the sense of a probability of belonging to one of $j$ classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05feec9b-d152-49cc-ab49-cac1477d02c2",
   "metadata": {},
   "source": [
    "$$ A(z_1) +  A(z_2)+  A(z_3) = \\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}} + \\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}}+ \\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}} =  1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17d9c9-cb14-4bdb-9b9d-cf4322f1b69b",
   "metadata": {},
   "source": [
    "In contrast to the output activation of networks for classification, a **linear activation function** is used in the **output layer of regression networks** in order to obtain continuous values that are not restricted to a specific interval. The linear activation function can be written as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6ab380-5180-4194-94db-3ae03b955897",
   "metadata": {},
   "source": [
    "$$A(z) = z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5fe545-b5f0-4599-8d31-3e16a85e3c51",
   "metadata": {},
   "source": [
    "The most important **activation functions** are summarized again in the following illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44828fc6-03d4-46db-a405-94e764606097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate values\n",
    "x = np.linspace(-6, 6, 100)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "relu = np.maximum(0, x)\n",
    "tanh = np.tanh(x)\n",
    "linear = x\n",
    "softmax = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "leaky_relu = np.where(x > 0, x, 0.01 * x)  # Leaky ReLU mit Alpha = 0.01\n",
    "\n",
    "# Create plot\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.tight_layout()\n",
    "\n",
    "# Sigmoid activation function\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(x, sigmoid, label=\"Sigmoid\", color=\"b\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.title(\"Sigmoid activation function\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# ReLU activation function\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(x, relu, label=\"ReLU\", color=\"r\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.title(\"ReLU activation function\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Tanh activation function\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(x, tanh, label=\"Tanh\", color=\"g\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.title(\"Tanh activation function\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Linear activation function\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(x, linear, label=\"Lineare\", color=\"m\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.title(\"Linear activation function\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Softmax activation function\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(x, softmax, label=\"Softmax\", color=\"c\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.title(\"Softmax activation function\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Leaky ReLU activation function\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.plot(x, leaky_relu, label=\"Leaky ReLU\", color=\"y\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.title(\"Leaky ReLU activation function (Alpha=0.01)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88bfefd-ab87-4152-bc72-fc01946baba7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simple neural network with activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343eb253-e0e1-4bc5-bcf8-f32e65149890",
   "metadata": {},
   "source": [
    "Let's apply activation functions to the network outlined above. We use **ReLU activation** in the first and second hidden layer and **Sigmoid activation** in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b06f4-a48b-4419-bb02-4e6786168acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_func(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf434b18-6928-4e3b-af59-a016152ce48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return (1/(1 + np.exp(1)**(-X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beebe2b9-4f03-4a93-9914-86b2b1bfac5d",
   "metadata": {},
   "source": [
    "### Feedforward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9072f6d-64ac-46ec-92cc-0676cdf5956a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initializing the weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bad954-27e6-4409-bfe6-534a15128e7a",
   "metadata": {},
   "source": [
    "We are trying to create a neural network from the theoretical considerations so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e001098-68ee-4b79-8174-2747bd15e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3 # input layer\n",
    "W1_size    = 3 # 3 neurons in the 1st hidden layer - ReLU activation\n",
    "W2_size    = 4 # 4 neurons in the 2nd hidden layer - ReLU activation\n",
    "W3_size    = 2 # 2 output classes - sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea06ec7-5b66-4bcb-ac91-80cd2abdfd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array([[5],[7],[1]]) # Input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7801fe7c-362c-42de-8164-fdbcc67548df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random weights and biases\n",
    "W1 = np.random.rand(W1_size, input_size) - 0.5\n",
    "b1 = np.random.randn(input_size, 1)\n",
    "W2 = np.random.rand(W2_size, W1_size) - 0.5\n",
    "b2 = np.random.rand(W2_size, 1) - 0.5\n",
    "W3 = np.random.rand(W3_size, W2_size) - 0.5\n",
    "b3 = np.random.rand(W3_size, 1) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe04c0-e8cf-44c0-9760-b79f7288bf07",
   "metadata": {},
   "source": [
    "### Output hidden layer $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4c398-06de-4b50-a858-5e131c3de431",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e9d48-e032-44a8-8845-a6039fcd4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfbd291-cb9c-4519-a8c5-1c14d3392833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the first hidden layer with ReLU activation\n",
    "A1 = relu_func(W1 @ X + b1)\n",
    "A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe6e36a-ed76-4fb2-9920-8675b605d28d",
   "metadata": {},
   "source": [
    "### Output hidden layer $2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b97d95-f84c-4190-8911-1f688ae34cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc2cfd-ba11-4f2a-898b-5f62d9f7bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13014191-bce4-4701-aa4f-bc9e347fce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the second hidden layer with ReLU activation\n",
    "A2 = relu_func(W2 @ A1 + b2)\n",
    "A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009cc2f3-771c-41e4-a85b-efcbea79bee0",
   "metadata": {},
   "source": [
    "### Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7f42a-fed4-4f86-8f3d-d893ec2cf708",
   "metadata": {},
   "outputs": [],
   "source": [
    "W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1948d9e-42da-4222-ba42-652751ac7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909d26e-4aa1-4144-82cd-e355729e8484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer with sigmoid activation\n",
    "A3 = sigmoid(W3 @ A2 + b3)\n",
    "A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7434855d-f5c2-4543-9015-3d3e432415b9",
   "metadata": {},
   "source": [
    "## Numerical derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784fc7f8-23c8-41da-a0f5-b0f53c66bd11",
   "metadata": {},
   "source": [
    "In preparation for the **backpropagation algorithm**, we will briefly deal with the numerical derivation of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0071c-84bd-4d52-9ad4-0f8f94b7079e",
   "metadata": {},
   "source": [
    "As an example, let's look at the function $f(x) = x^5 + x^3 + x$ and its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030a175-dbb7-4760-90e1-a9dabe81f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**5 + x**3 + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee3e117-7b3a-4fb8-bee7-3eaeb948c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure: f(x) = x**5 + x**3 + x\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "x = np.linspace(-2,2, 1000)\n",
    "y = f(x)\n",
    "\n",
    "\n",
    "_ = plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6411943-e058-4b60-8a7b-05e489bfb5a3",
   "metadata": {},
   "source": [
    "### Difference quotient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2816b-bcb5-4429-a23f-dea12bc9b870",
   "metadata": {},
   "source": [
    "To derive the function numerically, we can use the **difference quotient** $\\frac{ f (x + \\epsilon) - f (x)}{(x + \\epsilon) - x }$ of the function at the points $x$ and $x + \\epsilon$ as the simplest approximation, where $\\epsilon$ denotes the step size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164bef6d-a525-4608-a54d-b104fa3e97cf",
   "metadata": {},
   "source": [
    "$$f^{\\prime}(x) = \\frac{d f (x)}{d x}  \\approx \\frac{ f (x + \\epsilon) - f (x)}{(x + \\epsilon) - x } = \\frac{ f (x + \\epsilon) - f (x)}{\\epsilon} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a9ba9-d63b-4079-8860-43770516d0f0",
   "metadata": {},
   "source": [
    "$$eg.: f^{\\prime}(x) = (x^2)^{\\prime} = 2x  \\approx \\frac{ (x + \\epsilon)^2 - (x)^2}{\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e6fe6-0f41-47e8-9ba3-56f6f11d5e45",
   "metadata": {},
   "source": [
    "We can write this in `Python` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd776f-c4e4-4a04-98b1-34210c77aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(f, x, delta = 10**-5):\n",
    "    return (f(x + delta) - f(x))/delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a8d0d-690c-40ca-ba0f-680344aeca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure: f(x), f'(x)\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.grid()\n",
    "plt.xlim([-2.2,2.2])\n",
    "plt.ylim([-2.2,7.2])\n",
    "plt.yticks(np.arange(-2, 8, 1))\n",
    "plt.xticks(np.arange(-2, 3, 1))\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.tight_layout()\n",
    "plt.plot(x,y)\n",
    "_ = plt.plot(x, derivative(f, x, 10**-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4d0ae-2ca2-4258-afad-30b4e66ad063",
   "metadata": {},
   "source": [
    "### Derivation of the ReLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d204530a-6877-468f-a7a8-88743215febd",
   "metadata": {},
   "source": [
    "The ReLU function is given by $f_{ReLU}(x) = max(0, x)$. Let's try to numerically derive this important and perhaps somewhat unintuitive function with our `derivative()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fc73f-5c9a-4aa0-b2bd-9277d729cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_func(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d647b6-619a-4628-b261-72a7ef16f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(relu_func, x, delta = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53af458-7ed6-42b6-a781-d2bc7d538651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.tight_layout()\n",
    "plt.plot(x, derivative(relu_func, x, delta = 1e-5), linewidth = 2.5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"H(x)\")\n",
    "plt.title(\"Heaviside step function as a derivative of the ReLU function\")\n",
    "plt.ylim([-0.5,1.5])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e94d78-1f50-4592-b32f-f29e30e2b852",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Numerical derivative problem - discontinuous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c00a67-e894-4c03-88d6-5f271946a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure: ReLU function and derivative of the ReLU function\n",
    "# ReLU function and numerical derivative\n",
    "def relu_func(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivative(func, x, delta=1e-5):\n",
    "    return (func(x + delta) - func(x)) / (delta)\n",
    "\n",
    "# Function that updates the plot\n",
    "def plot_reAct(num_points):\n",
    "    x = np.linspace(-2, 2, num_points)  # Adjust the number of points\n",
    "\n",
    "    plt.figure(dpi=600, figsize=(6, 3))\n",
    "    plt.rcParams.update({'font.size': 6})\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ReLU function\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, relu_func(x), label='ReLU(x)', color='blue')\n",
    "    plt.title('ReLU function')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('ReLU(x)')\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Numerical derivation of the ReLU function\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, derivative(relu_func, x, delta=1e-5), label=\"ReLU'(x)\", color='red')\n",
    "    plt.title('Numerical derivation of the ReLU function')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel(\"ReLU'(x)\")\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Slider for the number of points displayed\n",
    "interactive_plot = interactive(plot_reAct, num_points=(70, 1000, 1))  # Schieberegler von 70 bis 1000 Punkten\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621de76-b8fb-44df-86a7-97cf8494dd76",
   "metadata": {},
   "source": [
    "A simpler solution is not to form the actual derivative, but to define the derivative using the Heaviside function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d7a66-5cb2-4ccc-a915-6e0b07905677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_ReLU(Z):\n",
    "    return np.where(Z > 0, 1, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764fe18f-780e-403a-8376-1e6045b7288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2, 1000)\n",
    "plt.title('The Heaviside function as a derivative of the ReLU function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"ReLU'(x)\")\n",
    "_ = plt.plot(x, derivative_ReLU(x), color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6834a6ec-7962-4fda-9bc9-ba9bc460e239",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Using the function `derivative(f, x, delta = 10**-5)` defined above, derive the function $f(x) = sin(x) \\cdot cos(x)$. And plot the derivative in the range $[-5, 5]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832986e-5f71-429a-8e6f-e8b869124069",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_werte = np.linspace(-5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc4322-6b70-4de7-8ac5-49839cfaad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91fb18-f964-4c52-840d-5be5f8b4564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = plt.plot(x, derivative(f, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c507e9e-b2cf-4560-be6b-2de6d493eaf2",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41abbc-7053-4ca8-b5ec-a3718bb35176",
   "metadata": {},
   "source": [
    "The classical **[gradient descend](https://en.wikipedia.org/wiki/Gradient_descent)** is about determining a global maximum/minimum of a given function in the ideal case; this is also referred to as a so-called **[optimization problem](https://en.wikipedia.org/wiki/Mathematical_optimization)**. The procedure is to form the **[gradient](https://en.wikipedia.org/wiki/Gradient)** of a function for all independent parameters. The gradient points in the direction of the largest increase in the function. To arrive at the minimum of the function, we choose a starting value $w_{\\text{old}}$, which serves as the starting point of the iterative descent to the minimum, and at each step of the gradient procedure we go in the direction of the negative gradient of the function by subtracting the gradient times a learning rate $\\alpha$ from $w_{\\text{old}}$ to calculate $w_{\\text{new}}$. In general, you can write:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b2df63-264d-4a81-b1d3-3dd27a6d6e9f",
   "metadata": {},
   "source": [
    "$$w_{\\text{new}} = w_{\\text{old}} - \\alpha \\cdot \\nabla f(w) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea36786-c4e4-4842-af87-2854d3361896",
   "metadata": {},
   "source": [
    "Here, $\\nabla$ is the **[Nabla operator](https://en.wikipedia.org/wiki/Del)** also called **Del operator**. In the one-dimensional case, it simply corresponds to the derivative with respect to the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616f86e-d58e-48bd-9866-068c5056a8d9",
   "metadata": {},
   "source": [
    "For example, let's consider determining the minimum of the function $f(x)= x^2$ to understand the gradient descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5faa2-144f-48c6-820e-f91b2890988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "x = np.linspace(-5, 5, N)\n",
    "y = x**2\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.tight_layout()\n",
    "_ = plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22d400-913b-489b-abbd-5845ee55ca27",
   "metadata": {},
   "source": [
    "We determine the gradient of the function $f(x)=x^2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d599df0-c97e-485b-976f-cf82d3c0ce1d",
   "metadata": {},
   "source": [
    "$$\\frac{df}{dx} = 2 x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909dd24a-7486-4f6a-bae4-c5bc6fa4694f",
   "metadata": {},
   "source": [
    "To determine the minimum, we subtract the gradient multiplied by a learning rate $\\alpha$ from a randomly selected starting value and iterate until the gradient converges to $0$. We thus move in the opposite direction of the largest increase in the function with each iteration towards a minimum. In each step, we calculate the next $x$ value:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88836c-7541-42a2-9f51-8aed92af2abd",
   "metadata": {},
   "source": [
    "$$x_{\\text{new}} = x_{\\text{old}} - \\alpha \\cdot \\frac{df}{dx} = x_{\\text{old}} - \\alpha \\cdot 2 x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af259d-fc75-4741-b346-419533427eaa",
   "metadata": {},
   "source": [
    "Let's take a look at a code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749b7f1-45c5-4f2d-a16c-02cc1bee71b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_old = 5\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30ffa4-c5c6-43df-b7d4-04303e95e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 30):\n",
    "    x_new = x_old - alpha * (2 * x_old)\n",
    "    x_old = x_new\n",
    "x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2dd94-c5f2-45dd-968e-1c0ff349de37",
   "metadata": {},
   "source": [
    "As we can see, the value for $x_{\\text{new}}$ converges towards the minimum of $f(x)$ at $x = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0632ed29-d614-4772-811c-8680c0afe3b9",
   "metadata": {},
   "source": [
    "The following figure shows the first three steps in the gradient descent method with a learning rate of `alpha = 0.1` and starting from the starting point `x_old = 5`. Note that the step size decreases the closer we get to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd63da-d50b-423b-bb1b-1013f1b0ffdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Objective function and its derivative\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "# Starting point and learning rate for the gradient descent\n",
    "x_start = 5.0\n",
    "alpha = 0.1\n",
    "\n",
    "# Number of steps\n",
    "num_steps = 3\n",
    "x_history = [x_start]\n",
    "\n",
    "# Perform gradient descent and draw the arrows\n",
    "for _ in range(num_steps):\n",
    "    x_current = x_start\n",
    "    x_start = x_start - alpha * df(x_start)\n",
    "    x_history.append(x_start)\n",
    "\n",
    "# X values for the function\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Create diagram\n",
    "plt.figure(dpi=600, figsize=(6, 3))\n",
    "plt.plot(x, f(x), label=\"f(x) = $x^2$\", color=\"blue\")\n",
    "_ = plt.scatter(\n",
    "    x_history, [f(x) for x in x_history], color=\"red\", label=\"Gradient Descent Steps\"\n",
    ")\n",
    "\n",
    "# Draw arrows connecting the steps of the gradient descent\n",
    "for i in range(1, len(x_history)):\n",
    "    dx = x_history[i] - x_history[i - 1]\n",
    "    dy = f(x_history[i]) - f(x_history[i - 1])\n",
    "    plt.quiver(\n",
    "        x_history[i - 1],\n",
    "        f(x_history[i - 1]),\n",
    "        dx,\n",
    "        dy,\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=1,\n",
    "        color=\"green\",\n",
    "        width=0.0075,\n",
    "        headaxislength=4,\n",
    "        headlength=4,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.legend()\n",
    "plt.title(\"Gradient Descent\")\n",
    "plt.grid(True)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c206a45-5035-493c-8cd9-8e70af129406",
   "metadata": {},
   "source": [
    "The convergence speed (the necessary number of steps) depends on the learning rate and the randomly selected starting point. The aim is to find a middle way between a learning rate that is **too low**, which leads to an unnecessarily **high number of steps**, and a learning rate that is **too high**, which leads to **oscillating solutions** as it repeatedly jumps over the minimum. Often, finding the best learning rate is an iterative process of experimentation. You can start with different learning rate settings (e.g. $0.1$, $0.01$, $0.001$) and monitor the performance on a validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81326a5d-4425-4e94-9ae4-af6a8df7bc86",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let us now try to apply the gradient descent method to train a neural network and consider the following example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ef6d7-2eb0-4d5a-9d00-0a4527be2b49",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c26d8-ea7c-4cea-a4ce-3a2fc537c74d",
   "metadata": {},
   "source": [
    "Let's look at an example of the **backpropagation algorithm** step by step. For the sake of clarity, we will use a network with two inputs, a hidden layer with two neurons and **linear activation** and an output layer with **sigmoid activation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a89ff0-94c3-4ef6-9902-8a8a3afd3b70",
   "metadata": {},
   "source": [
    "## Forwardpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe45b56-82e3-452f-b73d-f7cc3c62b416",
   "metadata": {},
   "source": [
    "First, we carry out a forward propagation for the neural network shown in the figure as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22dbe1f-45b0-42d6-aa4e-88c8b990058d",
   "metadata": {},
   "source": [
    "<img src=\"./images_en/backprop_forward.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07edbf15-beb6-4e82-9131-351d95f5133a",
   "metadata": {},
   "source": [
    "### Initial parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c67af-ce0f-4014-b8fb-175b69ecedbd",
   "metadata": {},
   "source": [
    "We assume the following initial parameters for weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc1198-7aec-4778-83df-81810529f1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X1 = 0.85\n",
    "\n",
    "X2 = 0.5\n",
    "\n",
    "w1 = 0.75\n",
    "\n",
    "w2 = 0.55\n",
    "\n",
    "w3 = 0.05\n",
    "\n",
    "w4 = 0.05\n",
    "\n",
    "w5 = 0.05\n",
    "\n",
    "w6 = 0.015\n",
    "\n",
    "w7 = 0.85\n",
    "\n",
    "w8 = 0.95\n",
    "\n",
    "b1 = 0.25\n",
    "\n",
    "b2 = 0.15\n",
    "\n",
    "b3 = 0.15\n",
    "\n",
    "b4 = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2eccf-fda3-4819-8e3b-4725b50306c1",
   "metadata": {},
   "source": [
    "We use linear activation for the input and hidden layers and the sigmoid activation function for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17782b54-2f69-4765-a506-35dfd2df202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return (1/(1 + np.exp(1)**(-X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994abc1-6620-4d56-afbd-e33c9071c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b32ef-513d-412e-939e-113f677a7940",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6), dpi = 600)\n",
    "x_werte = np.linspace(-20,20,1000)\n",
    "ax.set_title('Sigmoid activation function', fontsize = 14)\n",
    "_ = ax.plot(x_werte, sigmoid(x_werte))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98bc68-de30-4c21-8545-eb3456f4be11",
   "metadata": {},
   "source": [
    "The derivative of the sigmoid function is given by\n",
    "\n",
    "$S(x)^{\\prime} = S(x) (1 - S(x)) $\n",
    "\n",
    "We can check this by deriving the sigmoid function with our function `derivative()` and plotting the result simultaneously with the above expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8bf8f-98a2-432e-bbad-39b7e31c4841",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6), dpi = 600)\n",
    "ax.set_title('Derivation of the sigmoid function', fontsize = 14)\n",
    "#plt.plot(x_werte, sigmoid(x_werte)*(1 - sigmoid(x_werte)))\n",
    "_ = ax.plot(x_werte, derivative(sigmoid, x_werte, 10**-12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f1364-bc3f-4f9c-b635-a6e1a7b74605",
   "metadata": {},
   "source": [
    "We calculate the forward pass through the network up to the activations $A_3$, $A_4$ in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb030d-1097-48ed-b4a4-63063abb5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = w1 * X1 + w2 * X2 + b1\n",
    "Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d50c7-e89c-43cb-8d93-898117a60389",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2 = w3 * X1 + w4 * X2 + b2\n",
    "Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4be92c-aa3e-44a0-94ce-457f1cfe63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = linear_activation(Z1)\n",
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63485af-4144-41a2-86c8-41cbcae8e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = linear_activation(Z2)\n",
    "A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec64827-e548-4060-867e-584c6b6f41d9",
   "metadata": {},
   "source": [
    "#### Activation in the output neuron $1$: $A_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14984b4-f604-452f-a9c9-1daca87ebbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z3 = w5 * A1 + w6 * A2 + b3\n",
    "Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadcf710-5718-4478-9437-026f353bf1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A3 = sigmoid(Z3)\n",
    "A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e7ad5-e124-4bde-b5b1-e271375a59a2",
   "metadata": {},
   "source": [
    "#### Activation in the output neuron $2$: $A_4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a804e-3380-4e0c-ba51-2db14ad8e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z4 = w7 * A1 + w8 * A2 + b4\n",
    "Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1aac4-884e-49b4-be94-e1aeb3c1b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A4 = sigmoid(Z4)\n",
    "A4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad876ef0-2b09-45b0-92d5-c9245d996526",
   "metadata": {},
   "source": [
    "###  Loss function - Mean Squared Error (MSE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78f88fa-01da-4d7b-9199-19b4b94419f5",
   "metadata": {},
   "source": [
    "First, we need to introduce a measure for the error of the model. One way to evaluate the error of a model is given by the **[MSE (Mean Squared Error)](https://en.wikipedia.org/wiki/Mean_squared_error)**. In general, we speak of **[loss functions](https://en.wikipedia.org/wiki/Loss_function)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c29a6f-7b7a-41cf-9670-8aa890730275",
   "metadata": {},
   "source": [
    "$$ MSE = E_{total} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat y_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3aa57-d424-4b5e-8ce3-ff6ce9b1cfe4",
   "metadata": {},
   "source": [
    "Where $y_i$ is the expected output (*ground truth*) and $\\hat y_i$ is the prediction of the model (*prediction*). We assume $\\hat y_1 = 0.01$ and $\\hat y_2 = 0.99$ are given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676cd483-a342-4f2e-91f5-6694f40d100e",
   "metadata": {},
   "source": [
    "The index $n$ runs across all neurons in the output layer. In our example, $E_{total} = E_1 + E_2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683db27-ac96-45f5-a263-0f76b30ebf6d",
   "metadata": {},
   "source": [
    "$E_1 = \\frac{1}{2} (\\hat y_1 - A_3 )^2 = \\frac{1}{2} (0.01 - A_3 )^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3e195-077d-4103-8a9a-54b6250c7b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error output layer - first neuron\n",
    "E_1 = 1/2*(0.01 - A3)**2 \n",
    "E_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72543af-d893-4ae5-afe9-9a3752f1f900",
   "metadata": {},
   "source": [
    "$E_2 = \\frac{1}{2} (\\hat y_2 - A_4 )^2 = \\frac{1}{2} (0.99 - A_4)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b748e0e1-6192-430d-9f0d-bb672ab71d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error output layer - second neuron\n",
    "E_2 = 1/2*(0.99 - A4)**2 \n",
    "E_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ecf12-ce96-481d-a197-643d40c647e5",
   "metadata": {},
   "source": [
    "$E_{total} = E_1 + E_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe635a-9748-4a2b-ab72-9636ba522dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_total = E_1 + E_2\n",
    "E_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf90d1-4d9a-4aad-b419-4ac8d1cb888f",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5799e9e-4865-490d-bf86-2d65393b83f2",
   "metadata": {},
   "source": [
    "We have therefore given the total error of the output by $E_{total}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0a1cc-3292-438d-9932-810560cdc65e",
   "metadata": {
    "citation-manager": {
     "citations": {
      "0t31g": [
       {
        "id": "16738657/H9BKTPET",
        "source": "zotero"
       }
      ],
      "c4fkk": [
       {
        "id": "16738657/P59K4ZW6",
        "source": "zotero"
       }
      ],
      "k1ov4": [
       {
        "id": "16738657/GXMJ3Q3D",
        "source": "zotero"
       }
      ],
      "kbsyk": [
       {
        "id": "16738657/ZZNR2L5E",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "Let us now turn to the optimization method for this error, the **[backpropagation algorithm](https://en.wikipedia.org/wiki/Backpropagation)**. To do this, we will use the basics we discussed earlier, such as the **gradient method** and **activation functions**, and try to systematically adjust the weights of the network in order to obtain better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e6c556-629e-44b7-89ad-0fb3f50708e0",
   "metadata": {},
   "source": [
    "We go backwards from the outputs of the neural network and adjust the weights and biases in the direction of the gradient descent method. In order to calculate the dependencies of the error (the loss function), we have to apply the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba03c5-375f-46c9-bf2d-e759e2e5299b",
   "metadata": {},
   "source": [
    "As a reminder, let's look at the application of the chain rule using an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5174af1-7afd-4845-aacc-e32097125842",
   "metadata": {},
   "source": [
    "### Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945adfa8-81b9-4b6f-a47a-99a0f291121a",
   "metadata": {},
   "source": [
    "$f(g(x))^{\\prime} = \\frac{df}{dg}\\frac{dg}{dx}$\n",
    "\n",
    "e.g.:\n",
    "\n",
    "$f = g^2, g = sin(x)$\n",
    "\n",
    "$\\frac{d}{dx}(sin(x))^2 = 2 \\cdot sin(x) \\cdot cos(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78617849-022a-4c6d-a18e-a0438bd2d3e1",
   "metadata": {},
   "source": [
    "## Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ac026-0436-432d-ba54-1d4ed075e50a",
   "metadata": {},
   "source": [
    "Starting from the output layer, let's calculate the adjustment of the weights ($w_5, w_6, w_7, w_8$) of the output layer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca1b20-9f96-4e98-9faa-19614e015b7d",
   "metadata": {},
   "source": [
    "<img src=\"./images_en/backprop_w5_f2.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf9acb7-bbae-432b-b0d6-b69512ac7aa8",
   "metadata": {},
   "source": [
    "### Partial derivative of $E_{total}$ with respect to (w.r.t.) $w_5$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2633b3c-8a2a-46b7-9009-a27d93e4fbe7",
   "metadata": {},
   "source": [
    "#### Total derivation: $\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial w_5}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d6350-c30d-48d3-89d9-cca5c90650c2",
   "metadata": {},
   "source": [
    "#### 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc05960a-de7a-40d0-afc5-e04fce59df20",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial E_{total}}{\\partial A_3} = \\frac{\\partial }{\\partial A_3}(E_1 + E_2 ) = \\frac{\\partial }{\\partial A_3}(\\frac{1}{2} (\\hat y_1 - A_3 )^2 + \\frac{1}{2} (\\hat y_2 - A_4 )^2 ) = \\frac{\\partial }{\\partial A_3}(\\frac{1}{2} ( 0.01 - A_3 )^2 + \\frac{1}{2} (0.99 - A_4 )^2 )  \\\\ = -(0.01 - A_3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1892c62-c917-4f27-b530-fa6cf01c6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_A3 = -(0.01 - A3)\n",
    "dE_total_wrt_A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09b8ea-03a5-415a-b4e6-3c750adfc2d4",
   "metadata": {},
   "source": [
    "#### 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e07a25-a6c6-427c-abc3-107c46638ad7",
   "metadata": {},
   "source": [
    "####  $\\frac{\\partial A_3}{\\partial Z_3} = -\\frac{\\partial }{\\partial Z_3} \\frac{1}{1 + e^{-Z_3}} = A_3 (1 - A_3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a8fcc-abc9-4d39-8b33-38accf69da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA3_wrt_Z3 = A3 * (1 - A3)\n",
    "dA3_wrt_Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f4afe-ddc7-45d4-b010-78df15f2160f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622b8028-6af9-4d18-815c-7ac28ea9e95b",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial Z_3}{\\partial w_5} = \\frac{\\partial }{\\partial w_5}(w_5 \\cdot A_1 + w_6 \\cdot A_2 + b_3) = A_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d6ee3-b528-4d1d-8337-08898b2a8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ3_wrt_w5 = A1\n",
    "dZ3_wrt_w5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315c32a-214c-4316-84d2-75f4f863d476",
   "metadata": {},
   "source": [
    "#### Total derivation: $\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial w_5}   $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b10b3b-1597-4814-84c7-f8d83b497e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_w5 = dE_total_wrt_A3 * dA3_wrt_Z3 * dZ3_wrt_w5\n",
    "dE_total_wrt_w5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f0577-3206-4563-809c-1550d1a10128",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adjust weight $w_5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd62f37-c63c-4577-9e08-f63a3022a754",
   "metadata": {},
   "source": [
    "$w_{5 neu} = w_5 - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial w_5}$, $\\alpha = 0.5 \\cdots \\text{Learning rate}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4572de-8304-40b3-ac67-eb25a14673d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w5_new = w5 - 0.5 * dE_total_wrt_w5\n",
    "w5_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c88ce06-d3f6-40c5-96b9-b4607b6d7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in w5\n",
    "delta_w5 = w5_new - w5\n",
    "delta_w5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1850bf-2ab8-4020-b8ff-bee50fa54707",
   "metadata": {},
   "source": [
    "### Partial derivative of $E_{total}$ w.r.t. $w_6$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966890bd-e39c-4067-8654-dbdde24e3e14",
   "metadata": {},
   "source": [
    "<img src=\"./images_en/backprop_w6_f3.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349dc189-e0f5-461f-ab9e-70e5ca96fb21",
   "metadata": {},
   "source": [
    "#### Total derivation: $\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial w_6}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58daa8-a041-411e-bf42-75d505736609",
   "metadata": {},
   "source": [
    "#### 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a9569d-16cd-4d54-8b36-244a7221115c",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial E_{total}}{\\partial A_3} = -(0.01 - A_3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71a587-920b-4553-add3-0c6cc59cf6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_A3 = -(0.01 - A3)\n",
    "dE_total_wrt_A3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62021e-5e6c-42a5-a8ea-43c8cd673339",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec85d5d0-a2e4-446f-9c0f-e91ebcc32482",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  $\\frac{\\partial A_3}{\\partial Z_3} = -\\frac{\\partial }{\\partial Z_3} \\frac{1}{1 + e^{-Z_3}} = A_3 (1 - A_3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec8800-9cc6-40bd-98b3-8769146275b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA3_wrt_Z3 = A3 * (1 - A3)\n",
    "dA3_wrt_Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd3b615-5927-4eae-878c-42e1b1f0ed91",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2357fe7-a956-4160-b495-e0fc5419c3b7",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial Z_3}{\\partial w_6} = \\frac{\\partial }{\\partial w_6}(w_5 \\cdot A_1 + w_6 \\cdot A_2 + b_3) = A_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ffa10-fc85-451e-a3ee-1dc35060ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ3_wrt_w6 = A2\n",
    "dZ3_wrt_w6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda57f45-7f7e-4e10-aec2-3628e47a9c94",
   "metadata": {},
   "source": [
    "#### Total deriviation: $\\frac{\\partial E_{total}}{\\partial w_6} = \\frac{\\partial E_{total}}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial w_6}   $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093616b0-9e8a-4948-8c3c-812ea56f5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_w6 = dE_total_wrt_A3 * dA3_wrt_Z3 * dZ3_wrt_w6\n",
    "dE_total_wrt_w6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef38d0-85a0-43ab-8284-fb5258f6ff17",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adjust weight $w_6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128ecd2-368a-42d3-8aba-a4ca3e1428c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w6_new = w6 - 0.5 * dE_total_wrt_w6\n",
    "w6_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b80bb-ce7a-40a1-aa9b-2ed199d227c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in w6\n",
    "delta_w6 = w6_new - w6\n",
    "delta_w6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb7429-eb8d-4081-aa2d-2f2a6f8f0a4a",
   "metadata": {},
   "source": [
    "### Partial derivative of $E_{total}$ w.r.t. $w_7$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad865949-9081-45aa-969e-1addaf2dc570",
   "metadata": {},
   "source": [
    "<img src=\"./images_en/backprop_w7_f.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1535da-589d-4d91-b3ca-9ebd5189a3c9",
   "metadata": {},
   "source": [
    "#### Total derivation: $\\frac{\\partial E_{total}}{\\partial w_7} = \\frac{\\partial E_{total}}{\\partial A_4} \\frac{\\partial A_4}{\\partial Z_4} \\frac{\\partial Z_4}{\\partial w_7}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf08e2-01c0-4a93-967d-472bc0760f7e",
   "metadata": {},
   "source": [
    "#### 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c28d802-2924-43d0-85ff-7f1264e1efee",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial E_{total}}{\\partial A_4}  =\\frac{\\partial }{\\partial A_4}(\\frac{1}{2} (\\hat y_1 - A_3 )^2 + \\frac{1}{2} (\\hat y_2 - A_4 )^2 ) \\\\ = \\frac{\\partial }{\\partial A_4}(\\frac{1}{2} ( 0.01 - A_3 )^2 + \\frac{1}{2} (0.99 - A_4 )^2 )  \\\\ = -(0.99 - A_4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344dba6-0c85-4174-8b56-e528b76f2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_A4 = -(0.99 - A4)\n",
    "dE_total_wrt_A4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470ae65-9253-4ba3-9101-b6da8565017e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8151a8-c180-4874-a9a7-5b8449207bc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  $\\frac{\\partial A_4}{\\partial Z_4} = \\frac{\\partial }{\\partial Z_4} \\frac{1}{1 + e^{-Z_4}} = A_4 (1 - A_4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf2a72-8a8f-4895-af2f-321477a4e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA4_wrt_Z4 = A4 * (1 - A4)\n",
    "dA4_wrt_Z4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651f9d9-6238-42e4-863f-f6e6883a387b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273cf56-6302-41f7-a8d0-b31c355ab4ac",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial Z_4}{\\partial w_7} = \\frac{\\partial }{\\partial w_7}(w_7 \\cdot A_1 + w_8 \\cdot A_2 + b_4) = A_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc922c9c-59a0-4219-a398-f6d2bcb7806c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dZ4_wrt_w7 = A1\n",
    "dZ4_wrt_w7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e8a212-f1e1-4250-8a3f-c352c48b1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_w7 = dE_total_wrt_A4 * dA4_wrt_Z4 * dZ4_wrt_w7\n",
    "dE_total_wrt_w7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc877eb8-d044-4d27-9c22-fca53ff9ccc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adjust weight $w_7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ea30a-00ca-4e78-9b92-bf3772ce83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w7_new = w7 - 0.5 * dE_total_wrt_w7\n",
    "w7_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d537e04-1446-421a-a46c-029fed602d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in w7\n",
    "delta_w7 = w7_new - w7\n",
    "delta_w7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08cedbe-efa5-42c3-8f98-770326ae4549",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Partial derivative of $E_{total}$ w.r.t. $w_8$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba2a998-3b8e-4765-888a-6658a5a66b75",
   "metadata": {},
   "source": [
    "<img src=\"./images_en/backprop_w8_f.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ff94f-654d-4d1d-b4cb-27069069a1d2",
   "metadata": {},
   "source": [
    "#### Total derivation: $\\frac{\\partial E_{total}}{\\partial w_8} = \\frac{\\partial E_{total}}{\\partial A_4} \\frac{\\partial A_4}{\\partial Z_4} \\frac{\\partial Z_4}{\\partial w_8}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b4263-1732-4504-96cc-0da94657f6bc",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Calculate the partial derivative of $E_{total}$ w.r.t. $w_8$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d4451-5252-4df8-8c0f-048632f8eff7",
   "metadata": {},
   "source": [
    "#### 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48f9f5-5fe9-4f13-a8e6-c89ae6ebaaf8",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial E_{total}}{\\partial A_4} = -(0.99 - A_4)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d1f4e-3a53-4577-a992-564138c61e9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c569d8-2645-4493-b3f9-1f656581682d",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  $\\frac{\\partial A_4}{\\partial Z_4} = \\frac{\\partial }{\\partial Z_4} \\frac{1}{1 + e^{-Z_4}} = A_4 (1 - A_4)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf67e3-c58a-4fdc-9ee4-468899520ae5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585d12d-d4ab-4b73-ad94-dfc002e9597c",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial Z_4}{\\partial w_8} = \\frac{\\partial }{\\partial w_8}(w_7 \\cdot A_1 + w_8 \\cdot A_2 + b_2) = A_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fa7a5-3b2e-40e7-8dd7-03e11b1d99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed0b1d-6ec0-416f-b3db-5f99f7e34a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca634e-c349-48d4-a202-ef2936e6bc03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cb214-d63e-4003-b36b-efb7ba71ee60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee8108-823a-488d-b8c6-8306bafa1143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c4949-e59b-4c94-a1b9-d52899006b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1952c1a-fe1f-4918-9ec6-b5d002f525d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_A4 = -(0.99 - A4)\n",
    "dE_total_wrt_A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb24b4-937d-440c-a54c-ab6a4606a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA4_wrt_Z4 = A4 * (1 - A4)\n",
    "dA4_wrt_Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9069ae2c-3dd1-415c-a6e8-24ce2a9e47e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dZ4_wrt_w8 = A2\n",
    "dZ4_wrt_w8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ce637-46ae-43f5-a87b-e56024137abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_w8 = dE_total_wrt_A4 * dA4_wrt_Z4 * dZ4_wrt_w8\n",
    "dE_total_wrt_w8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36faa6b5-c8b2-4d37-aac0-0e6992b0419d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adjust weight $w_8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89638cbe-cff3-4576-9f4f-ca1fe1a9412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w8_new = w8 - 0.5 * dE_total_wrt_w8\n",
    "w8_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5e6cc-f285-49bb-8ba7-3015ace70123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in w8\n",
    "delta_w8 = w8_new - w8\n",
    "delta_w8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd173383-e021-415b-a3b9-efac69fcab46",
   "metadata": {},
   "source": [
    "## Hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de543d58-0431-43c2-ad42-451aff76a1a9",
   "metadata": {},
   "source": [
    "In the hidden layer, there is an additional dependency since both $E_1$ and $E_2$ depend on $A_1$ and $A_2$, resulting in $E_{total}$: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaca086-8908-4245-8ec7-e71f4fdaf915",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Partial derivative of $E_{total}$ w.r.t. $w_1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d823c4-4f41-47fd-b4b4-786ac3c928a4",
   "metadata": {},
   "source": [
    "<img src=\"./images_en/backprop_w1_f.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca824e1-b1b8-478d-aa11-e68e2daf2ed0",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial A_1} \\frac{\\partial A_1}{\\partial Z_1} \\frac{\\partial Z_1}{\\partial w_1}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f178b-5166-49a1-8085-5faf2a31fb4d",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial A_1} = \\frac{\\partial E_1}{\\partial A_1} +  \\frac{\\partial E_2}{\\partial A_1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5936bfa-0d69-4e88-80bc-e7886469bede",
   "metadata": {},
   "source": [
    "therefore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8bbd03-81d7-4d48-82c9-093ef00198ed",
   "metadata": {},
   "source": [
    "$E_1 = \\frac{1}{2} (\\hat y_1 - A_3 )^2 \\\\ = \\frac{1}{2} (0.01 - A_3 )^2 \\\\ = \\frac{1}{2} (0.01 - sigmoid(Z3) )^2 \\\\ = \\frac{1}{2} (0.01 - sigmoid(w_5 \\cdot A_1 + w_6 \\cdot A_2 + b_2) )^2 \\\\ = E_1 (A_1, A_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cf9310-4310-4201-b2bf-a2c1787c5673",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd6406f-8e63-4139-ab82-3fe9a4f30a55",
   "metadata": {},
   "source": [
    "$E_2 = \\frac{1}{2} (\\hat y_2 - A_4 )^2 \\\\ = \\frac{1}{2} (0.99 - A_4 )^2 \\\\ = \\frac{1}{2} (0.99 - sigmoid(Z4) )^2 \\\\ = \\frac{1}{2} (0.99 - sigmoid(w_7 \\cdot A_1 + w_8 \\cdot A_2 + b_2) )^2 \\\\ = E_2 (A_1, A_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e4d81-c961-4ad1-91b5-1573a57d08d1",
   "metadata": {},
   "source": [
    "#### 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135df65-3d64-4da5-9202-a7efa62d7521",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_1}{\\partial A_1} = \\frac{\\partial E_1}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial A_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742bf115-e8ef-461a-9ac8-a462e00a1577",
   "metadata": {},
   "source": [
    "We have already calculated $\\frac{\\partial E_1}{\\partial Z_3}$ since the following applies:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec710d-b414-4ebc-b870-2bfea5c3cbb7",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_1}{\\partial Z_3} = \\frac{\\partial E_1}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f9f5f-3707-4069-9559-6bfe866b4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE1_wrt_Z3 = dE_total_wrt_A3 * dA3_wrt_Z3\n",
    "dE1_wrt_Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86038d9b-4bca-46e1-be23-6cb93138f962",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_3}{\\partial A_1}$ results in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3985289-62a7-4176-89d0-671a374e35bb",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_3}{\\partial A_1} = \\frac{\\partial }{\\partial A_1} (w_5 A_1 + w_6 A_2 + b_2) = w_5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f404584-e995-4438-ab04-29925b183811",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ3_wrt_A1 = w5\n",
    "dZ3_wrt_A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e184263-ef88-4002-9830-0084286ce67d",
   "metadata": {},
   "source": [
    "Overall, this results in $\\frac{\\partial E_1}{\\partial A_1} = \\frac{\\partial E_1}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial A_1}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b4ad2-5687-4d33-9a5d-30192dd5c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE1_wrt_A1 = dE1_wrt_Z3 * dZ3_wrt_A1\n",
    "dE1_wrt_A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fe9453-206c-4939-8654-17ba0a2a7dcb",
   "metadata": {},
   "source": [
    "#### 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb90a9d-3595-4a04-9938-78a6966d04e8",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_2}{\\partial A_1} = \\frac{\\partial E_2}{\\partial Z_4} \\frac{\\partial Z_4}{\\partial A_1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7091d19-250b-45fa-8f08-b2efb49fccc2",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_1}{\\partial Z_4} = \\frac{\\partial E_1}{\\partial A_4} \\frac{\\partial A_4}{\\partial Z_4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050a70c-2257-4a1f-857c-2dc9e650d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE2_wrt_Z4 = dE_total_wrt_A4 * dA4_wrt_Z4\n",
    "dE2_wrt_Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab27e57-22a7-4141-b128-159485fbcf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ4_wrt_A1 = w7\n",
    "dZ4_wrt_A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490000b4-c5e3-4206-8a2a-e2dc34541bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE2_wrt_A1 = dE2_wrt_Z4 * dZ4_wrt_A1\n",
    "dE2_wrt_A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08094b-f4d5-4406-9ba2-5bfb8d5b2dbc",
   "metadata": {},
   "source": [
    "In total, this results in $\\frac{\\partial E_{total}}{\\partial A_1} = \\frac{\\partial E_1}{\\partial A_1} + \\frac{\\partial E_2}{\\partial A_1} $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d549be64-61e8-47b1-aed7-b3f66f1f019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_A1 = dE1_wrt_A1 + dE2_wrt_A1\n",
    "dE_total_wrt_A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b31aec-b70c-4cae-a853-fce8c90b629b",
   "metadata": {},
   "source": [
    "Let's focus on the initial equation again: $\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial A_1} \\frac{\\partial A_1}{\\partial Z_1} \\frac{\\partial Z_1}{\\partial w_1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb5af54-140b-48e4-b7b8-f76194fda2a5",
   "metadata": {},
   "source": [
    "We still need $\\frac{\\partial A_1}{\\partial Z_1}$ and $\\frac{\\partial Z_1}{\\partial w_1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc83e9-3719-4d8d-b7e5-f2586da4ec26",
   "metadata": {},
   "source": [
    "$\\frac{\\partial A_1}{\\partial Z_1} = A_1 (1 - A_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a23fc-1aa6-42f5-b9bb-ab9df6a35c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA1_wrt_Z1 = A1 * (1 - A1)\n",
    "dA1_wrt_Z1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6d489-0659-4e99-967d-9cae19a28593",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_1}{\\partial w_1} = \\frac{\\partial Z_1}{\\partial w_1} (w_1 X_1 + w_2 X_2 + b_1) = X_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543bb0f5-ae9c-4141-9ee0-6c2ebad7e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ1_wrt_w1 = X1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044b99fc-e319-468c-9b82-73fa6ca7c8e6",
   "metadata": {},
   "source": [
    "Overall, this results in $\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial E_{total}}{\\partial A_1} \\frac{\\partial A_1}{\\partial Z_1} \\frac{\\partial Z_1}{\\partial w_1} $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4f14a-2ad1-44b3-b094-92b58525ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_w1 = dE_total_wrt_A1 * dA1_wrt_Z1 * dZ1_wrt_w1\n",
    "dE_total_wrt_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8425315-9c44-4dad-ab97-48ebe414da97",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adjust weight $w_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364756e-ba39-40f5-a68d-a78622d9a87b",
   "metadata": {},
   "source": [
    "We can now adjust the weight $w_1$ with $w_{1 new} = w_1 - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial w_1}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913f522-5430-44e7-917a-64d58cc935c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_new = w1 - 0.5 * dE_total_wrt_w1\n",
    "w1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b90c9-67da-418b-9ef6-fab220f81fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in w1\n",
    "delta_w1 = w1_new - w1\n",
    "delta_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307e9a02-01d5-4f0f-aae0-10a3f13bf7ea",
   "metadata": {},
   "source": [
    "### Partial derivative of $E_{total}$ w.r.t. $w_2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a9907-7efe-4fc1-be6a-68d7b53c50ac",
   "metadata": {},
   "source": [
    "<img src=\"./images_en/backprop_w2_f.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbbc0f-5319-4b0d-a349-a62563397718",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial w_2} = \\frac{\\partial E_{total}}{\\partial A_1} \\frac{\\partial A_1}{\\partial Z_1} \\frac{\\partial Z_1}{\\partial w_2}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea892e4f-60a0-46b9-86c5-a603653e634c",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial A_1} = \\frac{\\partial E_1}{\\partial A_1} +  \\frac{\\partial E_2}{\\partial A_1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d794d6-adee-4a87-9625-9c7d0b1bc46e",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_1}{\\partial w_2} = \\frac{\\partial Z_1}{\\partial w_2} (w_1 X_1 + w_2 X_2 + b_1) = X_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f8806-e0be-43a8-9c97-4370d904843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ1_wrt_w2 = X2\n",
    "dZ1_wrt_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a6a274-c37f-4394-a8f8-67db742fe6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_w2 = dE_total_wrt_A1 * dA1_wrt_Z1 * dZ1_wrt_w2\n",
    "dE_total_wrt_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c936a1-d1dc-457c-9e09-1c87f2f8a496",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adjust weight $w_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3b6b6-50f5-4180-bb08-71fcb0debeac",
   "metadata": {},
   "source": [
    "We can now adjust the weight $w_2$ with $w_{2 new} = w_2 - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial w_2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92251cee-c8cf-4d74-a753-3427bc99fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_new = w2 - 0.5 * dE_total_wrt_w2\n",
    "w2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c43481-00de-46bc-bfe6-2134d956ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in w2\n",
    "delta_w2 = w2_new - w2\n",
    "delta_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65887e7d-177c-4e61-8f31-c06692802ed5",
   "metadata": {},
   "source": [
    "### Partial derivative of $E_{total}$ w.r.t. $w_3$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e46d3-d694-440b-936d-d26ad5c8ab4c",
   "metadata": {},
   "source": [
    "<img src=\"./images_en/backprop_w3_f.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4cbf0f-8353-4553-a522-8a22d349b945",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial A_2} \\frac{\\partial A_2}{\\partial Z_2} \\frac{\\partial Z_2}{\\partial w_3}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87423bd-c57d-477e-85e4-b12404052447",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial A_2} = \\frac{\\partial E_1}{\\partial A_2} +  \\frac{\\partial E_2}{\\partial A_2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18a7cf-8218-43d1-8370-4af646ff5966",
   "metadata": {},
   "source": [
    "#### 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb63f78-c0df-484e-b0dd-95d886a1d226",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_1}{\\partial A_2} = \\frac{\\partial E_1}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial A_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7026ee14-7f98-4c59-a448-f986c7a9d317",
   "metadata": {},
   "source": [
    "We have already calculated $\\frac{\\partial E_1}{\\partial Z_3}$ since the following applies:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55029b67-c52c-469e-9b20-5ca3dea0fbfe",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_1}{\\partial Z_3} = \\frac{\\partial E_1}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ef0a8c-b678-468b-9830-1ac647cbb09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE1_wrt_Z3 = dE_total_wrt_A3 * dA3_wrt_Z3\n",
    "dE1_wrt_Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5833253c-0a18-447c-8d43-d3a6b73d971f",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_3}{\\partial A_2}$ results in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30651880-30ad-4bc5-96b1-bf9217d14566",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_3}{\\partial A_2} = \\frac{\\partial }{\\partial A_2} (w_5 A_1 + w_6 A_2 + b_2) = w_6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74dc99-4006-457c-beac-71ba82e0a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ3_wrt_A2 = w6\n",
    "dZ3_wrt_A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10b642-996e-4f1d-bc50-9f7923f72cda",
   "metadata": {},
   "source": [
    "In total, this results in $\\frac{\\partial E_1}{\\partial A_2} = \\frac{\\partial E_1}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial A_2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20a43be-563b-4c87-8ebe-f3e8c02d038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE1_wrt_A2 = dE1_wrt_Z3 * dZ3_wrt_A2\n",
    "dE1_wrt_A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c94ba-83e7-4294-a9db-9fef1dd36c7a",
   "metadata": {},
   "source": [
    "#### 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160bb4c4-2671-43a5-b17d-7bfd66ad3ded",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_2}{\\partial A_2} = \\frac{\\partial E_2}{\\partial Z_4} \\frac{\\partial Z_4}{\\partial A_2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196a3f2a-6eba-414d-bd7d-3c71f615d649",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_2}{\\partial Z_4} = \\frac{\\partial E_2}{\\partial A_4} \\frac{\\partial A_4}{\\partial Z_4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8356dca2-f88e-4703-af37-42be722bd3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE2_wrt_Z4 = dE_total_wrt_A4 * dA4_wrt_Z4\n",
    "dE2_wrt_Z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ef015-8993-433d-b613-6fd3ee55ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ4_wrt_A2 = w8\n",
    "dZ4_wrt_A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91bd732-1eac-43c0-bdf9-eba99b56d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE2_wrt_A2 = dE2_wrt_Z4 * dZ4_wrt_A2\n",
    "dE2_wrt_A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0643cfe-e1c4-47c1-ae93-672898aa7ad2",
   "metadata": {},
   "source": [
    "In total, this results in $\\frac{\\partial E_{total}}{\\partial A_2} = \\frac{\\partial E_1}{\\partial A_2} + \\frac{\\partial E_2}{\\partial A_2} $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef6afb-4b9c-462b-af4b-ce6eedcc39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_A2 = dE1_wrt_A2 + dE2_wrt_A2\n",
    "dE_total_wrt_A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b7803-1060-45fd-9602-4581ee8d643e",
   "metadata": {},
   "source": [
    "Let's focus on the initial equation again: $\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial A_2} \\frac{\\partial A_2}{\\partial Z_2} \\frac{\\partial Z_2}{\\partial w_3} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b406f-967e-490a-aa0b-073ab5fb8dea",
   "metadata": {},
   "source": [
    "We still need $\\frac{\\partial A_2}{\\partial Z_2}$ and $\\frac{\\partial Z_2}{\\partial w_3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c71a94-b513-43f2-9480-3e9c51c5e0a8",
   "metadata": {},
   "source": [
    "$\\frac{\\partial A_2}{\\partial Z_2} = A_2 (1 - A_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c7f65-9bba-4aa2-b611-bff1dd0b3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA2_wrt_Z2 = A2 * (1 - A2)\n",
    "dA2_wrt_Z2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63579c28-04df-464e-9dcf-b4f98fa98b73",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_2}{\\partial w_3} = \\frac{\\partial }{\\partial w_3} (w_3 X_1 + w_4 X_2 + b_1) = X_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e755b-41ed-47d2-8095-0792a1aa5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ2_wrt_w3 = X1\n",
    "dZ2_wrt_w3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b54435-8b27-4e9f-8436-bfcef927c327",
   "metadata": {},
   "source": [
    "Overall, the result for $\\frac{\\partial E_{total}}{\\partial w_3} = \\frac{\\partial E_{total}}{\\partial A_2} \\frac{\\partial A_2}{\\partial Z_2} \\frac{\\partial Z_2}{\\partial w_3} $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c462a957-2700-412e-9332-57362d9300cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_w3 = dE_total_wrt_A2 * dA2_wrt_Z2 * dZ2_wrt_w3\n",
    "dE_total_wrt_w3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52febdd4-6b5f-4ec4-887c-3f8ffe9a647c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adjust weight $w_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c724681-75de-4a97-bab3-3e42206048a0",
   "metadata": {},
   "source": [
    "We can now adjust the weight $w_3$ with $w_{3 new} = w_3 - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial w_3}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5754f8-c3ee-4ed1-97da-a56b8d9736c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w3_new = w3 - 0.5 * dE_total_wrt_w3\n",
    "w3_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7709b-1759-4128-a9f9-03137206bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in w3\n",
    "delta_w3 = w3_new - w3\n",
    "delta_w3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691c5c3-c564-481a-9bf6-afc4f3e573b8",
   "metadata": {},
   "source": [
    "### Partial derivative of $E_{total}$ w.r.t. $w_4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f013218-68fc-42c5-bd0f-cf6d1f95ac28",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Calculate the partial derivative of $E_{total}$ w.r.t. $w_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a1ee52-4998-4da3-9c48-0dc0fffc69f6",
   "metadata": {},
   "source": [
    "<img src=\"./images_en/backprop_w4_f.png\" alt=\"drawing\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763d20c-d88e-4ba2-abe5-8cfd3d612c89",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial w_4} = \\frac{\\partial E_{total}}{\\partial A_2} \\frac{\\partial A_2}{\\partial Z_2} \\frac{\\partial Z_2}{\\partial w_4}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504f16d-b64e-4994-8262-b0101d87ab0e",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E_{total}}{\\partial A_2} = \\frac{\\partial E_1}{\\partial A_2} +  \\frac{\\partial E_2}{\\partial A_2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c40c4-a94e-464b-bbc2-49662c93fd43",
   "metadata": {},
   "source": [
    "$\\frac{\\partial Z_2}{\\partial w_4} = \\frac{\\partial Z_2}{\\partial w_4} (w_3 X_1 + w_4 X_2 + b_1) = X_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782a939-2d57-4e0f-aaf9-4697e0d9a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa619d8-89b9-47c1-bcbe-9c5be276ff1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fcffce-1941-484e-826d-22a30f02d686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c07a8-7de4-4139-b388-61fb22c7970c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97919ff-88ef-474f-826e-ebdd37da119d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1ff46-705b-47bc-8951-ced3865a151e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e609e-e07c-479d-97df-be527eeb3e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0909e-1822-4d2a-babf-8f9f4176936e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b23d850-cb36-4c9c-806a-c263457ec3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12534c90-faad-4c3a-a14c-c46dcc1d9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ2_wrt_w4 = X2\n",
    "dZ2_wrt_w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221cdf7-38b3-4b54-b78f-3f4756389eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dE_total_wrt_w4 = dE_total_wrt_A2 * dA2_wrt_Z2 * dZ2_wrt_w4\n",
    "dE_total_wrt_w4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c81601-3f9c-4463-bf75-aceafe7fa7c8",
   "metadata": {},
   "source": [
    "We can now adjust the weight $w_4$ with $w_{4 new} = w_4 - \\alpha \\cdot \\frac{\\partial E_{total}}{\\partial w_4}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21e57f-b01e-48f2-a6ea-7c3accd0eb98",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adjust weight $w_4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f461966-7cd3-4380-8f8d-7d45c45ead17",
   "metadata": {},
   "outputs": [],
   "source": [
    "w4_new = w4 - 0.5 * dE_total_wrt_w4\n",
    "w4_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9647de-d812-4c7a-9aa0-48fccdf8f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in w4\n",
    "delta_w4 = w4_new - w4\n",
    "delta_w4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
